{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "root_folder = Path(os.getcwd())\n",
    "dataset_dir = root_folder / \"data/har-up-spiking-dataset-240\"\n",
    "\n",
    "multiclass = True\n",
    "batch_size = 4\n",
    "hidden_layers = [64,32]\n",
    "nb_steps = 3000\n",
    "time_duration = 60\n",
    "tau_mem = 100\n",
    "tau_syn = 50\n",
    "\n",
    "last_layer_size = 12 if multiclass else 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169234,)\n"
     ]
    }
   ],
   "source": [
    "from utils.SpikingDataset import SpikingDataset\n",
    "\n",
    "dataset = SpikingDataset(\n",
    "    root_dir=dataset_dir,\n",
    "    time_duration=time_duration,\n",
    "    camera1_only=False,\n",
    "    multiclass=multiclass\n",
    ")\n",
    "train_dataset, dev_dataset, test_dataset = dataset.split_by_trials()\n",
    "\n",
    "events, target = test_dataset[0]\n",
    "print(events.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DataLoader of size 679\n",
      "Initializing DataLoader of size 66\n",
      "Initializing DataLoader of size 372\n"
     ]
    }
   ],
   "source": [
    "from utils.SpikingDataLoader import SpikingDataLoader\n",
    "\n",
    "train_loader = SpikingDataLoader(dataset=train_dataset,nb_steps=nb_steps,batch_size=batch_size,shuffle=False)\n",
    "dev_loader = SpikingDataLoader(dataset=dev_dataset, nb_steps=nb_steps, batch_size=batch_size, shuffle=False)\n",
    "test_loader = SpikingDataLoader(dataset=test_dataset, nb_steps=nb_steps, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init SpikingNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.SpikingNN import SpikingNN\n",
    "\n",
    "model = SpikingNN(\n",
    "    layer_sizes=[240 * 180] + hidden_layers + [last_layer_size],\n",
    "    nb_steps=nb_steps,\n",
    "    time_step=time_duration / nb_steps,\n",
    "    tau_mem=tau_mem * 1e-3,\n",
    "    tau_syn=tau_syn * 1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Leaky Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.SNNTorchLeaky import SNNTorchLeaky\n",
    "\n",
    "model = SNNTorchLeaky(\n",
    "    num_inputs=dataset.nb_pixels,\n",
    "    num_hidden=250,\n",
    "    num_outputs=2,\n",
    "    nb_steps=nb_steps,\n",
    "    time_step=time_duration / nb_steps,\n",
    "    tau_mem=tau_mem * 1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Synaptic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.SNNTorchSyn import SNNTorchSyn\n",
    "\n",
    "model = SNNTorchSyn(\n",
    "    num_inputs=dataset.nb_pixels,\n",
    "    num_hidden=250,\n",
    "    num_outputs=2,\n",
    "    nb_steps=nb_steps,\n",
    "    time_step=time_duration / nb_steps,\n",
    "    tau_mem=tau_mem * 1e-3,\n",
    "    tau_syn=tau_syn * 1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Convolutional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.SNNTorchConv import SNNTorchConv\n",
    "\n",
    "model = SNNTorchConv(\n",
    "    num_outputs=last_layer_size,\n",
    "    nb_steps=nb_steps,\n",
    "    time_step=time_duration / nb_steps,\n",
    "    tau_mem=tau_mem * 1e-3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(batch_size, nb_steps, 240, 180))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.BinaryTrainer import BinaryTrainer\n",
    "from utils.MultiTrainer import MultiTrainer\n",
    "\n",
    "if multiclass:\n",
    "    trainer = MultiTrainer(model=model)\n",
    "else:\n",
    "    trainer = BinaryTrainer(model=model)\n",
    "\n",
    "trainer.train(\n",
    "    train_loader,\n",
    "    evaluate_dataloader=dev_loader,\n",
    "    nb_epochs=5,\n",
    "    stop_early=False,\n",
    "    dataset_bias_ratio=5.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"B4_Conv_Leaky_local\"\n",
    "model_save_file = root_folder / \"models/saved\"\n",
    "\n",
    "model.save(model_save_file / f\"{model_name}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"B4_H64,32_N3000_LR25_W5_Multi\"\n",
    "model_save_file = root_folder / \"models/saved\"\n",
    "path = model_save_file / f\"{model_name}.pth\"\n",
    "\n",
    "model.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.BinaryTrainer import BinaryTrainer\n",
    "from utils.MultiTrainer import MultiTrainer\n",
    "\n",
    "if multiclass:\n",
    "    trainer = MultiTrainer(model=model)\n",
    "else:\n",
    "    trainer = BinaryTrainer(model=model)\n",
    "\n",
    "trainer = BinaryTrainer(model=model)\n",
    "trainer.test(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "last_layer_size = 12 if multiclass else 2\n",
    "timesteps_per_sec = nb_steps // time_duration\n",
    "\n",
    "# Initialize lists to store data from all batches\n",
    "x_locals_list = []\n",
    "y_preds_list  = []\n",
    "y_locals_list = []\n",
    "mem_recs_list = []\n",
    "spk_recs_list = []\n",
    "\n",
    "start_index = 0\n",
    "end_index = 400\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i, (x_local, y_local) in enumerate(test_loader):\n",
    "        if (start_index//batch_size <= i and i < end_index//batch_size):\n",
    "            x_local = x_local.to(model.device, model.dtype)\n",
    "            y_local = y_local.to(model.device, model.dtype)\n",
    "\n",
    "            mem, spk = model.forward(x_local.to_dense())\n",
    "            spk_reshaped = spk.reshape(-1, time_duration, timesteps_per_sec, last_layer_size).sum(dim=2)\n",
    "\n",
    "            # Get the max value for each second as the prediction\n",
    "            y_pred = torch.argmax(spk_reshaped, dim=2)\n",
    "            \n",
    "            # x_locals_list.append(x_local.to_dense().cpu().detach().numpy())\n",
    "            y_preds_list.append(y_pred.cpu().detach().numpy())\n",
    "            y_locals_list.append(y_local.cpu().detach().numpy())\n",
    "            # mem_recs_list.append(mem.cpu().detach().numpy())\n",
    "            # spk_recs_list.append(spk.cpu().detach().numpy())\n",
    "\n",
    "# x_locals = np.concatenate(x_locals_list, axis=0)\n",
    "y_preds  = np.concatenate(y_preds_list, axis=0)\n",
    "y_locals = np.concatenate(y_locals_list, axis=0)\n",
    "# mem_recs = np.concatenate(mem_recs_list, axis=0)\n",
    "# spk_recs = np.concatenate(spk_recs_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize set of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.snn_visualizers import plot_correctness_matrix, plot_confusion_matrix\n",
    "\n",
    "class_labels = {\n",
    "    0: \"No label\",\n",
    "    1: \"Falling forward using hands\",\n",
    "    2: \"Falling forward using knees\",\n",
    "    3: \"Falling backwards\",\n",
    "    4: \"Falling sideward\",\n",
    "    5: \"Falling sitting in empty chair\",\n",
    "    6: \"Walking\",\n",
    "    7: \"Standing\",\n",
    "    8: \"Sitting\",\n",
    "    9: \"Picking up an object\",\n",
    "    10: \"Jumping\",\n",
    "    11: \"Laying\"\n",
    "} if multiclass else {0:\"Not Fall\", 1:\"Fall\"}\n",
    "\n",
    "plot_confusion_matrix(y_locals, y_preds, class_labels)\n",
    "# plot_correctness_matrix(y_locals, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize specific sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.snn_visualizers import plot_predictions_and_labels, plot_snn_activity_combined, visualize_events\n",
    "\n",
    "index = 10\n",
    "\n",
    "visualize_events(x_locals[index], y_locals[index], time_duration)\n",
    "plot_snn_activity_combined(mem_recs[index], spk_recs[index])\n",
    "plot_predictions_and_labels(spk_recs[index], y_locals[index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
